# Scrapy PoC for Plural Open Scrapers

Proof of Concept for using Scrapy to write and execute scrapers that obtain open civic data.

## Try it

* Install the necessary version of python using `pyenv`
* If necessary, `pip install poetry`
* `poetry install`

You should now have the `scrapy` tool installed in your poetry environment.

**Warning**: currently something is wrong with the project structure. I can only run this with a modification to
PYTHONPATH (which PyCharm adds by default). How to set PYTHONPATH if you want to just run this in a terminal:

* Find the path to your Poetry environment folder: `poetry env info`
* Set PYTHONPATH to include both the root folder of this repo and the `site-packages` folder of the poetry env:
  `export PYTHONPATH='/home/jesse/repo/openstates/scrapy-test:/home/jesse/.cache/pypoetry/virtualenvs/scrapy-test-vH1KNbGC-py3.9/lib/python3.9/site-packages`
    * You will need to change the paths in the command above to match those on your machine.

### Run a scout scrape

`python -m scrapy.cmdline crawl nv-bills -a session=2023Special35 -O nv-bills-scout.json -s "ITEM_PIPELINES={}"`

* This command disables the `DropStubsPipeline` (`ITEM_PIPELINES={}`), which by default drops stub entities
* Results are output to `nv-bills-scout.json`

### Run a full scrape

`python -m scrapy.cmdline crawl nv-bills -a session=2023Special35 -O nv-bills.json -s "DOWNLOADER_MIDDLEWARES={}"`

* This command disables the `ScoutOnlyDownloaderMiddleware` (`DOWNLOADER_MIDDLEWARES={}`), which by default ignores
  requests that are not marked `is_scout` in the `meta` property of the request. 
* Results are output to `nv-bills.json`
* Please note that the scraper is not fully ported over yet, so there is still missing data.

## Context

### Why Scrapy?

* Very popular: easy to find developers who are familiar
* Very mature: battle-tested layers of abstraction, flexibility to meet our goals
* Reduce overall surface area of "in-house" code we need to maintain

### Criteria

One way to think of success for this project: can it achieve most or all of the goals of the spatula project, without
requiring much custom code?

* Can we run a scout scrape that returns basic info on entities without making the extra requests required for
  retrieving full info on entities?
* Can we run a normal scrape that does not output the partial/stub info generated by scout code?
* Are there barriers invovled in using necessary elements of `openstates-core` code here?
* Can the scraper output an equivalent directory of json files that can be compared 1:1 to an existing scraper?
