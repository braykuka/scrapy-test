# Scrapy PoC for Plural Open Scrapers

Proof of Concept for using Scrapy to write and execute scrapers that obtain open civic data.

## Try it

* Install the necessary version of python using `pyenv`
* If necessary, `pip install poetry`
* `poetry install`

You should now have the `scrapy` tool installed in your poetry environment.

**Warning**: currently something is wrong with the project structure. I can only run this with a modification to
PYTHONPATH (which PyCharm adds by default). How to set PYTHONPATH if you want to just run this in a terminal:

* Find the path to your Poetry environment folder: `poetry env info`
* Set PYTHONPATH to include both the root folder of this repo and the `site-packages` folder of the poetry env:
  `export PYTHONPATH='/home/jesse/repo/openstates/scrapy-test:/home/jesse/.cache/pypoetry/virtualenvs/scrapy-test-vH1KNbGC-py3.9/lib/python3.9/site-packages`
    * You will need to change the paths in the command above to match those on your machine.

### Run a scout scrape

`python -m scrapy.cmdline crawl nv-bills -a session=2023Special35 -O nv-bills-scout.json -s "ITEM_PIPELINES={}"`

* This command disables the `DropStubsPipeline` (`ITEM_PIPELINES={}`), which by default drops stub entities
* Results are output to `nv-bills-scout.json`

### Run a full scrape

`python -m scrapy.cmdline crawl nv-bills -a session=2023Special35 -O nv-bills.json -s "DOWNLOADER_MIDDLEWARES={}"`

* This command disables the `ScoutOnlyDownloaderMiddleware` (`DOWNLOADER_MIDDLEWARES={}`), which by default ignores
  requests that are not marked `is_scout` in the `meta` property of the request.
* Results are output to `nv-bills.json`
* Please note that the scraper is not fully ported over yet, so there is still missing data.

## Context

### Prior art

John did a scrapy PoC in the (private) [Plural Engineering Experiments repo](https://github.com/civic-eagle/data-engineering-experiements/tree/main/scrapy-test)

### Why Scrapy?

* Very popular: easy to find developers who are familiar
* Very mature: battle-tested layers of abstraction, flexibility to meet our goals
* Reduce overall surface area of "in-house" code we need to maintain

### Criteria

One way to think of success for this project: can it achieve most or all of the goals of the spatula project, without
requiring much custom code?

* Can we run a scout scrape that returns basic info on entities without making the extra requests required for
  retrieving full info on entities?
* Can we run a normal scrape that does not output the partial/stub info generated by scout code?
* Are there barriers involved in using necessary elements of `openstates-core` code here? For instance we want to be
  able to easily port code, and continue to use the core entity models w/ helper functions etc.
* Can the scraper output an equivalent directory of json files that can be compared 1:1 to an existing scraper?

## Technical notes

### Problems

* The `spatula` library specifies an older version of the `attrs` package as a dependency. `scrapy` also has `attrs` as
  a dependency. These versions conflict. And since `openstates` has `spatula` as a dependency, we currently cannot
  add `openstates` as a dependency to this project! To try to quickly work around this, I copied a bunch of library code
  out of the `openstates-core` repo and into the `core` package within this repo. This is a very temporary solution.
* Scraper is not fully ported


### Useful concepts

* Pass input properties from the comamnd line to the scraper using the `-a` flag, ie `-a session=2023Special35`. This
  allows us to copy `os-update` behavior where we can pass in runtime parameters to the scraper.
* Override scrapy settings at runtime with the `-s` flag. This allows us to set things like which Item pipelines and
  middleware is enabled at runtime. This allows us to switch the behavior between scout/normal scrape at runtime.
* Item pipelines do things with items returned by scrapers. Using this to drop "stub" items when in normal scrape mode.
* Downloader middleware allows us to change behavior of a Request before it is made. Currently requiring the scraper to
  mark `is_scout: True` on the `meta` property of the Request, so that we can ignore non-scout requests when desired.